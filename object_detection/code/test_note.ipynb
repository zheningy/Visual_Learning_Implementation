{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _init_paths\n",
    "from datasets.factory import get_imdb\n",
    "imdb = get_imdb('voc_2007_trainval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = imdb.roidb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gt_classes',\n",
       " 'boxscores',\n",
       " 'gt_vec',\n",
       " 'flipped',\n",
       " 'boxes',\n",
       " 'seg_areas',\n",
       " 'gt_overlaps']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,    4,    5, ..., 4000, 4001, 4002])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.where(a['gt_classes']==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gt_classes',\n",
       " 'boxscores',\n",
       " 'gt_vec',\n",
       " 'flipped',\n",
       " 'boxes',\n",
       " 'seg_areas',\n",
       " 'gt_overlaps']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "np.where(a['gt_classes']!=0)[0]\n",
    "a['gt_classes']\n",
    "a['gt_overlaps'][8,8]\n",
    "rois_blob = np.zeros((0, 5), dtype=np.float32)\n",
    "print(rois_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_2007_trainval gt roidb loaded from /home/yzn/CMU/18Spring/16824/hw2/hw2-release/code/data/cache/voc_2007_trainval_gt_roidb.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imdb.classes)\n",
    "imdb.classes\n",
    "imdb.num_images\n",
    "imdb.roidb[0]['gt_classes']\n",
    "ss = imdb.gt_roidb()[2018]['gt_classes']\n",
    "type(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aeroplane': 0,\n",
       " 'bicycle': 1,\n",
       " 'bird': 2,\n",
       " 'boat': 3,\n",
       " 'bottle': 4,\n",
       " 'bus': 5,\n",
       " 'car': 6,\n",
       " 'cat': 7,\n",
       " 'chair': 8,\n",
       " 'cow': 9,\n",
       " 'diningtable': 10,\n",
       " 'dog': 11,\n",
       " 'horse': 12,\n",
       " 'motorbike': 13,\n",
       " 'person': 14,\n",
       " 'pottedplant': 15,\n",
       " 'sheep': 16,\n",
       " 'sofa': 17,\n",
       " 'train': 18,\n",
       " 'tvmonitor': 19}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = imdb.classes\n",
    "class_to_idx = {}\n",
    "for item in classes:\n",
    "    class_to_idx[item] = classes.index(item)\n",
    "class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yzn/CMU/18Spring/16824/hw2/hw2-release/code/data/VOCdevkit2007/VOC2007/JPEGImages/003998.jpg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(imdb.image_index)\n",
    "img_name = imdb.image_path_at(2018)\n",
    "img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import visdom\n",
    "def vis_detections(im, class_name, dets, thresh=0.8):\n",
    "    \"\"\"Visual debugging of detections.\"\"\"\n",
    "    for i in range(np.minimum(10, dets.shape[0])):\n",
    "        bbox = tuple(int(np.round(x)) for x in dets[i, :4])\n",
    "        score = dets[i, -1]\n",
    "        if score > thresh:\n",
    "            cv2.rectangle(im, bbox[0:2], bbox[2:4], (0, 204, 0), 2)\n",
    "            cv2.putText(im, '%s: %.3f' % (class_name, score), (bbox[0], bbox[1] + 15), cv2.FONT_HERSHEY_PLAIN,\n",
    "                        1.0, (0, 0, 255), thickness=1)\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_2007_trainval gt roidb loaded from /home/yzn/CMU/18Spring/16824/hw2/hw2-release/code/data/cache/voc_2007_trainval_gt_roidb.pkl\n"
     ]
    }
   ],
   "source": [
    "gt_box = imdb.gt_roidb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 15 15 15]\n"
     ]
    }
   ],
   "source": [
    "print(gt_box[2]['gt_classes'])\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "with open(imdb.image_path_at(2), 'rb') as f:\n",
    "    img = Image.open(f)\n",
    "    img.convert('RGB')\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': array([[ 11,  59, 499, 306]], dtype=uint16),\n",
       " 'flipped': False,\n",
       " 'gt_classes': array([7], dtype=int32),\n",
       " 'gt_overlaps': <1x21 sparse matrix of type '<type 'numpy.float32'>'\n",
       " \twith 1 stored elements in Compressed Sparse Row format>,\n",
       " 'seg_areas': array([ 121272.], dtype=float32)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_name)\n",
    "res = vis_detections(img, 'car', gt_box['boxes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 500, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow('car',res)\n",
    "# cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 500, 375)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.transpose(2,1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 500, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_2007_trainval ss roidb loaded from /home/yzn/CMU/18Spring/16824/hw2/hw2-release/code/data/cache/voc_2007_trainval_selective_search_roidb.pkl\n"
     ]
    }
   ],
   "source": [
    "ss_box = imdb.selective_search_roidb()[2018]\n",
    "ss_box['boxscores'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1cc517bb1f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mss_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvis_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stuff'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss_box\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'car'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mss_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ss_res = vis_detections(img, 'stuff', ss_box['boxes'][-10:])\n",
    "cv2.imshow('car',ss_res)\n",
    "cv2.waitKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "model_urls = {\n",
    "        'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
    "}\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = model_zoo.load_url(model_urls['alexnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'features.0.weight',\n",
       " u'features.0.bias',\n",
       " u'features.3.weight',\n",
       " u'features.3.bias',\n",
       " u'features.6.weight',\n",
       " u'features.6.bias',\n",
       " u'features.8.weight',\n",
       " u'features.8.bias',\n",
       " u'features.10.weight',\n",
       " u'features.10.bias',\n",
       " u'classifier.1.weight',\n",
       " u'classifier.1.bias',\n",
       " u'classifier.4.weight',\n",
       " u'classifier.4.bias',\n",
       " u'classifier.6.weight',\n",
       " u'classifier.6.bias']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [x for x in model_dict.keys() if 'features' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {k: v for k, v in model_dict.items() if 'features' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'features.10.bias',\n",
       " u'features.10.weight',\n",
       " u'features.6.bias',\n",
       " u'features.6.weight',\n",
       " u'features.8.weight',\n",
       " u'features.3.bias',\n",
       " u'features.8.bias',\n",
       " u'features.0.weight',\n",
       " u'features.0.bias',\n",
       " u'features.3.weight']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalizerAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(LocalizerAlexNet, self).__init__()\n",
    "        #TODO: Define model\n",
    "        self.features = nn.Sequential(\n",
    "                            nn.Conv2d(3, 64, 11, stride=4, padding=2),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(3, stride=2),\n",
    "                            nn.Conv2d(64, 192, 5, stride=1, padding=2),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(3, stride=2),\n",
    "                            nn.Conv2d(192, 384, 3, stride=1, padding=1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(384, 256, 3, stride=1, padding=1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(256, 256, 3, stride=1, padding=1),\n",
    "                            nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "                        nn.Conv2d(256, 256, 3, stride=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(256, 256, 1, stride=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(256, num_classes, 1, stride=1)\n",
    "            )\n",
    "\n",
    "\n",
    "#         self.conv_1 = nn.Conv2d(3, 64, 11, stride=4, padding=2)\n",
    "#         self.conv_2 = nn.Conv2d(64, 192, 5, stride=1, padding=2)\n",
    "#         self.conv_3 = nn.Conv2d(192, 384, 3, stride=1, padding=1)\n",
    "#         self.conv_4 = nn.Conv2d(384, 256, 3, stride=1, padding=1)\n",
    "#         self.conv_5 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "\n",
    "#         self.conv6 = nn.Conv2d(256, 256, 3, stride=1)\n",
    "#         self.conv7 = nn.Conv2d(256, 256, 1, stride=1)\n",
    "#         self.conv8 = nn.Conv2d(256, num_classes, 1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO: Define forward pass\n",
    "        features = self.features(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), 3, stride=2)\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), 3, stride=2)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = F.relu(self.conv4(x))\n",
    "#         x = F.relu(self.conv5(x))\n",
    "#         x = F.relu(self.conv6(x))\n",
    "#         x = F.relu(self.conv7(x))\n",
    "#         return self.conv8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0,'/home/spurushw/reps/hw-wsddn-sol/faster_rcnn')\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import _init_paths\n",
    "from datasets.factory import get_imdb\n",
    "from custom import *\n",
    "from logger import *\n",
    "trainval_imdb = get_imdb('voc_2007_trainval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_2007_trainval gt roidb loaded from /home/yzn/CMU/18Spring/16824/hw2/hw2-release/code/data/cache/voc_2007_trainval_gt_roidb.pkl\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "train_dataset = IMDBDataset(\n",
    "        trainval_imdb,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize((512,512)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = None\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=(train_sampler is None),\n",
    "    num_workers=4, pin_memory=True, sampler=train_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.RandomSampler at 0x7f7d3b669f50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ouer', 123)\n"
     ]
    }
   ],
   "source": [
    "print(\"ouer\", 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
